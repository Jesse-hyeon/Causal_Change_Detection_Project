import torch
import torch.nn as nn

class Model(nn.Module):
    def __init__(self, configs):
        super(Model, self).__init__()

        self.num_classes = configs.c_out
        self.num_layers = configs.num_layers
        self.input_size = configs.input_size
        self.hidden_size = configs.hidden_size
        self.seq_length = configs.seq_len
        self.pred_len = configs.pred_len

        # LSTM ë„¤íŠ¸ì›Œí¬ ì •ì˜
        self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size,
                            num_layers=self.num_layers, batch_first=True)

        # ìµœì¢… ì˜ˆì¸¡ì„ ìœ„í•œ Linear ë ˆì´ì–´
        self.fc = nn.Linear(self.hidden_size, self.num_classes)

    def forward(self, x):
        batch_size = x.size(0)

        # ğŸŸ¢ ì´ˆê¸° hidden state ë° cell state ì •ì˜
        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)
        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)

        # ğŸŸ¢ LSTM ì‹¤í–‰
        lstm_out, _ = self.lstm(x, (h_0, c_0))  # (batch_size, seq_len, hidden_size)

        # ğŸŸ¢ LSTM ë§ˆì§€ë§‰ `pred_len` íƒ€ì„ìŠ¤í…ë§Œ ì„ íƒ
        lstm_out = lstm_out[:, -self.pred_len:, :]  # (batch_size, pred_len, hidden_size)

        # ğŸŸ¢ ìµœì¢… ì˜ˆì¸¡ê°’ ê³„ì‚°
        output = self.fc(lstm_out)  # (batch_size, pred_len, num_classes)

        return output